{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SMS Detection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from bs4 import BeautifulSoup\n",
    "import ipaddress\n",
    "from dateutil.parser import parse as date_parse\n",
    "from sklearn import metrics \n",
    "import warnings\n",
    "import requests\n",
    "from urllib.parse import urlparse, unquote, urljoin\n",
    "import re\n",
    "import socket\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53240,
     "status": "ok",
     "timestamp": 1702299094115,
     "user": {
      "displayName": "Sriyans Ketavarapu",
      "userId": "02252141626944290692"
     },
     "user_tz": -330
    },
    "id": "97n7MK-x5zz1",
    "outputId": "0319ede4-5820-4f25-a969-3404e02c97f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 18:53:09.999222: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 4s 50ms/step - loss: 0.2390 - accuracy: 0.9197 - val_loss: 0.0433 - val_accuracy: 0.9873\n",
      "Epoch 2/5\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 0.0202 - accuracy: 0.9946 - val_loss: 0.0344 - val_accuracy: 0.9900\n",
      "Epoch 3/5\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0063 - accuracy: 0.9991 - val_loss: 0.0482 - val_accuracy: 0.9900\n",
      "Epoch 4/5\n",
      "69/69 [==============================] - 3s 46ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0402 - val_accuracy: 0.9918\n",
      "Epoch 5/5\n",
      "69/69 [==============================] - 3s 47ms/step - loss: 5.1609e-04 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x35de1b410>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./SMS.csv')\n",
    "\n",
    "# Map labels to numerical values\n",
    "data['LABEL'] = data['LABEL'].map({'Smishing': 1, 'ham': 0})\n",
    "data.dropna(subset=['LABEL'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "# Prepare data for training\n",
    "X = data['TEXT']\n",
    "y = data['LABEL']\n",
    "print(y.unique())\n",
    "\n",
    "# Tokenize text\n",
    "max_words = 10000  # Define the maximum number of words to keep\n",
    "max_length = 200  # Define the sequence length\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_length)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "embedding_dim = 100\n",
    "filters = 128\n",
    "kernel_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))  # Two classes: 'Smishing' and 'ham'\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1702301256590,
     "user": {
      "displayName": "Sriyans Ketavarapu",
      "userId": "02252141626944290692"
     },
     "user_tz": -330
    },
    "id": "pZ605Fp_6JO6",
    "outputId": "7c7d7753-87c1-435b-bfa4-5d91ebf5f897"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#Save the trained model\n",
    "model.save('./sms_model5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n",
      "[[9.9998188e-01 1.8173008e-05]]\n",
      "0\n",
      "The model classifies the message as: 'ham'\n"
     ]
    }
   ],
   "source": [
    "# Custom message for classification\n",
    "# Load the saved model\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model('./sms_model5.h5')\n",
    "new_message = \"Its been so long since we talked. I miss you so much. Can you call me today?\"\n",
    "\n",
    "# Tokenize and pad the new message\n",
    "new_message_sequence = tokenizer.texts_to_sequences([new_message])\n",
    "new_message_padded = pad_sequences(new_message_sequence, maxlen=max_length)\n",
    "\n",
    "# Classify the new message\n",
    "prediction = loaded_model.predict(new_message_padded)\n",
    "print(prediction)\n",
    "predicted_label = np.argmax(prediction)\n",
    "print(predicted_label)\n",
    "# Decode the predicted label\n",
    "label_mapping = {0: 'ham', 1: 'Smishing'}\n",
    "predicted_class = label_mapping[predicted_label]\n",
    "\n",
    "print(f\"The model classifies the message as: '{predicted_class}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "[[0.00297017 0.9970298 ]]\n",
      "1\n",
      "The model classifies the message as: 'Smishing'\n"
     ]
    }
   ],
   "source": [
    "new_message = \"No Credit Score? No income proof? No Problem. Use your FD & get your HDFC Bank Credit Card today: https://hdfcbk.io/HDFCBK/s/dpwgW2YL T&C\"\n",
    "new_message_sequence = tokenizer.texts_to_sequences([new_message])\n",
    "new_message_padded = pad_sequences(new_message_sequence, maxlen=max_length)\n",
    "\n",
    "# Classify the new message\n",
    "prediction = loaded_model.predict(new_message_padded)\n",
    "print(prediction)\n",
    "predicted_label = np.argmax(prediction)\n",
    "print(predicted_label)\n",
    "# Decode the predicted label\n",
    "label_mapping = {0: 'ham', 1: 'Smishing'}\n",
    "predicted_class = label_mapping[predicted_label]\n",
    "\n",
    "print(f\"The model classifies the message as: '{predicted_class}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0CzajvF6iIy"
   },
   "source": [
    "<h1>Website content</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load zero-shot classifier (using Hugging Face Transformers)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_website_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "\n",
    "        # Extract visible text from body\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        body_text = ' '.join(p.get_text().strip() for p in paragraphs)\n",
    "        body_text = body_text[:2000]  # Limit to first 2000 characters for performance\n",
    "\n",
    "        return title, body_text, response.url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract from {url}: {e}\")\n",
    "        return \"\", \"\", url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_title_related_to_body(title, body):\n",
    "    if not title or not body:\n",
    "        return False\n",
    "\n",
    "    # Zero-shot classification to assess semantic relevance\n",
    "    hypothesis_template = f\"The webpage body content is related to the title: '{title}'\"\n",
    "    result = classifier(body, candidate_labels=[\"related\", \"not related\"], hypothesis_template=hypothesis_template)\n",
    "\n",
    "    # Return True if model thinks they are related\n",
    "    return result[\"labels\"][0] == \"related\" and result[\"scores\"][0] > 0.7\n",
    "\n",
    "def extract_features(url, title, body):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "\n",
    "    features = {\n",
    "        \"https\": url.startswith(\"https\"),\n",
    "        \"has_ip_address\": any(char.isdigit() for char in domain),\n",
    "        \"suspicious_words\": any(word in body.lower() for word in [\"login\", \"verify\", \"click here\", \"account suspended\"]),\n",
    "        \"short_title\": len(title) < 5,\n",
    "        \"low_title_body_similarity\": not is_title_related_to_body(title, body)\n",
    "    }\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_phishing_site(features):\n",
    "    # Rule-based decision making (can be replaced by ML model)\n",
    "    score = 0\n",
    "    score += 1 if not features[\"https\"] else 0\n",
    "    score += 1 if features[\"has_ip_address\"] else 0\n",
    "    score += 1 if features[\"suspicious_words\"] else 0\n",
    "    score += 1 if features[\"short_title\"] else 0\n",
    "    score += 1 if features[\"low_title_body_similarity\"] else 0\n",
    "\n",
    "    return score >= 3  # Threshold can be tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_url(url):\n",
    "    print(f\"\\n🔗 Analyzing: {url}\")\n",
    "    title, body, final_url = extract_website_content(url)\n",
    "    features = extract_features(final_url, title, body)\n",
    "    is_phishing = is_phishing_site(features)\n",
    "\n",
    "    print(f\"\\n📝 Title: {title}\")\n",
    "    print(f\"\\n📄 Body Sample: {body[:300]}...\\n\")\n",
    "    print(\"📊 Features Extracted:\")\n",
    "    for k, v in features.items():\n",
    "        print(f\"  - {k}: {v}\")\n",
    "\n",
    "    result = \"🚨 Phishing Site Detected!\" if is_phishing else \"✅ Legitimate Website.\"\n",
    "    print(f\"\\n🔍 Result: {result}\")\n",
    "    return is_phishing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Analyzing: https://www.google.com/\n",
      "Failed to extract from https://www.google.com/: HTTPSConnectionPool(host='www.google.com', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "📝 Title: \n",
      "\n",
      "📄 Body Sample: ...\n",
      "\n",
      "📊 Features Extracted:\n",
      "  - https: True\n",
      "  - has_ip_address: False\n",
      "  - suspicious_words: False\n",
      "  - short_title: True\n",
      "  - low_title_body_similarity: True\n",
      "\n",
      "🔍 Result: ✅ Legitimate Website.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example\n",
    "url = \"https://www.google.com/\"\n",
    "analyze_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Analyzing: https://www.amazon.in/\n",
      "Failed to extract from https://www.amazon.in/: HTTPSConnectionPool(host='www.amazon.in', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "📝 Title: \n",
      "\n",
      "📄 Body Sample: ...\n",
      "\n",
      "📊 Features Extracted:\n",
      "  - https: True\n",
      "  - has_ip_address: False\n",
      "  - suspicious_words: False\n",
      "  - short_title: True\n",
      "  - low_title_body_similarity: True\n",
      "\n",
      "🔍 Result: ✅ Legitimate Website.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example\n",
    "url = \"https://www.amazon.in/\"\n",
    "analyze_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Analyzing: https://www.cryptonitemit.in/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The provided hypothesis_template \"The webpage body content is related to the title: 'Cryptonite - Official Cybersecurity Student Project of MIT Manipal'\" was not able to be formatted with the target labels. Make sure the passed template includes formatting syntax such as {} where the label should go.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.cryptonitemit.in/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m analyze_url(url)\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36manalyze_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔗 Analyzing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m title, body, final_url \u001b[38;5;241m=\u001b[39m extract_website_content(url)\n\u001b[0;32m----> 4\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_features(final_url, title, body)\n\u001b[1;32m      5\u001b[0m is_phishing \u001b[38;5;241m=\u001b[39m is_phishing_site(features)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📝 Title: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(url, title, body)\u001b[0m\n\u001b[1;32m     13\u001b[0m parsed_url \u001b[38;5;241m=\u001b[39m urlparse(url)\n\u001b[1;32m     14\u001b[0m domain \u001b[38;5;241m=\u001b[39m parsed_url\u001b[38;5;241m.\u001b[39mnetloc\n\u001b[1;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m: url\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_ip_address\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28many\u001b[39m(char\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m domain),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuspicious_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28many\u001b[39m(word \u001b[38;5;129;01min\u001b[39;00m body\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclick here\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount suspended\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(title) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow_title_body_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;129;01mnot\u001b[39;00m is_title_related_to_body(title, body)\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mis_title_related_to_body\u001b[0;34m(title, body)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Zero-shot classification to assess semantic relevance\u001b[39;00m\n\u001b[1;32m      6\u001b[0m hypothesis_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe webpage body content is related to the title: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m classifier(body, candidate_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot related\u001b[39m\u001b[38;5;124m\"\u001b[39m], hypothesis_template\u001b[38;5;241m=\u001b[39mhypothesis_template)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Return True if model thinks they are related\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelated\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1294\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1297\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1298\u001b[0m             )\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter))\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/zero_shot_classification.py:209\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.preprocess\u001b[0;34m(self, inputs, candidate_labels, hypothesis_template)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, candidate_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hypothesis_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis example is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     sequence_pairs, sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(inputs, candidate_labels, hypothesis_template)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (candidate_label, sequence_pair) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(candidate_labels, sequence_pairs)):\n\u001b[1;32m    212\u001b[0m         model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_and_tokenize([sequence_pair])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/zero_shot_classification.py:29\u001b[0m, in \u001b[0;36mZeroShotClassificationArgumentHandler.__call__\u001b[0;34m(self, sequences, labels, hypothesis_template)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must include at least one label and at least one sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypothesis_template\u001b[38;5;241m.\u001b[39mformat(labels[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m hypothesis_template:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         (\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe provided hypothesis_template \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was not able to be formatted with the target labels. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure the passed template includes formatting syntax such as \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m}} where the label should go.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(hypothesis_template)\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sequences, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     37\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [sequences]\n",
      "\u001b[0;31mValueError\u001b[0m: The provided hypothesis_template \"The webpage body content is related to the title: 'Cryptonite - Official Cybersecurity Student Project of MIT Manipal'\" was not able to be formatted with the target labels. Make sure the passed template includes formatting syntax such as {} where the label should go."
     ]
    }
   ],
   "source": [
    "\n",
    "# Example\n",
    "url = \"https://www.cryptonitemit.in/\"\n",
    "analyze_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "local_path = \"./local-models/bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(local_path, local_files_only=True)\n",
    "model = BertModel.from_pretrained(local_path, local_files_only=True)\n",
    "\n",
    "def analyze_website_similarity(domain, threshold=0.6):\n",
    "    def extract_website_content(url):\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://' + url\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                return None\n",
    "        except requests.RequestException:\n",
    "            return None\n",
    "\n",
    "    def extract_title_and_body(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title = soup.title.text.strip() if soup.title else \"No title found\"\n",
    "        body = soup.body.get_text(separator=' ', strip=True) if soup.body else \"No body found\"\n",
    "        return title, body\n",
    "\n",
    "    def get_bert_embedding(text):\n",
    "        tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings\n",
    "\n",
    "    def check_similarity(title, body):\n",
    "        title_embedding = get_bert_embedding(title)\n",
    "        body_embedding = get_bert_embedding(body)\n",
    "        similarity_score = 1 - cosine(title_embedding, body_embedding)\n",
    "        return similarity_score\n",
    "\n",
    "    content = extract_website_content(domain)\n",
    "    if content:\n",
    "        title, body = extract_title_and_body(content)\n",
    "\n",
    "        if title == \"No title found\" and body == \"No body found\":\n",
    "            return {\"phishing_probability\": 1.0, \"message\": \"Title and body are empty.\"}\n",
    "        elif title == \"No title found\":\n",
    "            return {\"phishing_probability\": 1.0, \"message\": \"Title is empty.\"}\n",
    "        elif body == \"No body found\":\n",
    "            return {\"phishing_probability\": 1.0, \"message\": \"Body is empty.\"}\n",
    "\n",
    "        similarity = check_similarity(title, body)\n",
    "        phish_prob = 1 - similarity\n",
    "\n",
    "        message = \"Domain is unsafe\" if phish_prob >= threshold else \"Domain is safe\"\n",
    "        return {\"phishing_probability\": phish_prob, \"message\": message}\n",
    "    else:\n",
    "        return {\"phishing_probability\": 1.0, \"message\": \"Failed to extract website content.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phishing_probability': 0.2723313570022583, 'message': 'Domain is safe'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_website_similarity(\"https://www.cryptonitemit.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phishing_probability': 1.0, 'message': 'Failed to extract website content.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_website_similarity(\"https://www.irctc.co.in/nget/train-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phishing_probability': 0.3969181180000305, 'message': 'Domain is safe'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_website_similarity(\"https://www.fisglobal.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def analyze_website_similarity(domain, threshold=0.6):\n",
    "    local_path = \"./local-models/bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(local_path, local_files_only=True)\n",
    "    model = BertModel.from_pretrained(local_path, local_files_only=True)\n",
    "\n",
    "    def extract_website_content_with_selenium(url):\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://' + url\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.set_page_load_timeout(30)\n",
    "            driver.get(url)\n",
    "            time.sleep(5)  # Allow JS to load\n",
    "            html = driver.page_source\n",
    "            driver.quit()\n",
    "            return html\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching page with Selenium: {e}\"\n",
    "\n",
    "    def extract_title_and_body(html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title = soup.title.text if soup.title else \"No title found\"\n",
    "        body = soup.body.get_text(separator=' ', strip=True) if soup.body else \"No body found\"\n",
    "        return title, body\n",
    "\n",
    "    def get_bert_embedding(text):\n",
    "        tokens = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        return embeddings\n",
    "\n",
    "    def check_similarity(title, body):\n",
    "        title_embedding = get_bert_embedding(title)\n",
    "        body_embedding = get_bert_embedding(body)\n",
    "        similarity_score = 1 - cosine(title_embedding, body_embedding)\n",
    "        return similarity_score\n",
    "\n",
    "    html = extract_website_content_with_selenium(domain)\n",
    "    if isinstance(html, str) and html.startswith(\"Error\"):\n",
    "        return {\"phishing_probability\": 1, \"message\": html}\n",
    "\n",
    "    title, body = extract_title_and_body(html)\n",
    "\n",
    "    if title.strip() == \"No title found\" and body.strip() == \"No body found\":\n",
    "        return {\"phishing_probability\": 1, \"message\": \"Title and body are empty.\"}\n",
    "    elif title.strip() == \"No title found\":\n",
    "        return {\"phishing_probability\": 1, \"message\": \"Title is empty.\"}\n",
    "    elif body.strip() == \"No body found\":\n",
    "        return {\"phishing_probability\": 1, \"message\": \"Body is empty.\"}\n",
    "\n",
    "    similarity = check_similarity(title, body)\n",
    "    phish_prob = 1 - similarity\n",
    "    message = \"Domain is unsafe\" if phish_prob >= threshold else \"Domain is safe\"\n",
    "    \n",
    "    return {\n",
    "        \"phishing_probability\": round(phish_prob, 3),\n",
    "        \"similarity_score\": round(similarity, 3),\n",
    "        \"title\": title,\n",
    "        \"message\": message\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phishing_probability': 0.324, 'similarity_score': 0.676, 'title': 'www.irctc.co.in', 'message': 'Domain is safe'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.irctc.co.in/nget/train-search\"\n",
    "    result = analyze_website_similarity(url)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phishing_probability': 0.745, 'similarity_score': 0.255, 'title': 'Google', 'message': 'Domain is unsafe'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"google.com\"\n",
    "    result = analyze_website_similarity(url)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phishing_probability': 0.192, 'similarity_score': 0.808, 'title': 'Online Shopping site in India: Shop Online for Mobiles, Books, Watches, Shoes and More - Amazon.in', 'message': 'Domain is safe'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"amazon.in\"\n",
    "    result = analyze_website_similarity(url)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OCR Functionality</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqa5TPFtNKLgLZ6ZiyhAzN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
